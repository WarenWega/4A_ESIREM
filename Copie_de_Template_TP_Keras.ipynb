{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WarenWega/4A_ESIREM/blob/main/Copie_de_Template_TP_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les deux premiers blocs du Notebook ne sont pas à modifier. Ils servent à la configuration du projet."
      ],
      "metadata": {
        "id": "4nUBQLhc29BM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "74lvan3JxDnd"
      },
      "outputs": [],
      "source": [
        "# All the needed imports are already here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, concatenate, IntegerLookup, Normalization, StringLookup\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mandatory pre-processing functions\n",
        "# Converts a Pandas DataFram into a Keras Tensor Dataset\n",
        "\"\"\"\n",
        "    x:      Pandas Dataframe a dataframe containing all the subsets of data for the neural network\n",
        "    y:      Pandas Dataframe a dataframe containing all the labels for each subsets of data\n",
        "\"\"\"\n",
        "def dataframe_to_dataset(x, y):\n",
        "    train = x.copy()\n",
        "    labels = y\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(train), labels))\n",
        "    ds = ds.shuffle(buffer_size=len(train))\n",
        "    return ds\n",
        "\n",
        "# Converts a number input into an encoded tensor\n",
        "\"\"\"\n",
        "    feature:    any feature to encode\n",
        "    name:       string name of the feature\n",
        "    dataset:    Keras Tensor Dataset the dataset in which the feature is found\n",
        "\"\"\"\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "\n",
        "    return encoded_feature\n",
        "\n",
        "# Convert a categorical feature into an encoded tensor\n",
        "\"\"\"\n",
        "    feature:    any feature to encode\n",
        "    name:       string name of the feature\n",
        "    dataset:    Keras Tensor Dataset the dataset in which the feature is found\n",
        "    is_string:  bool whether the feature is a string\n",
        "\"\"\"\n",
        "def encode_categorical_feature(feature, name, dataset, is_string):\n",
        "    lookup_class = StringLookup if is_string else IntegerLookup\n",
        "    # Create a lookup layer which will turn strings into integer indices\n",
        "    lookup = lookup_class(output_mode=\"binary\")\n",
        "\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    lookup.adapt(feature_ds)\n",
        "\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = lookup(feature)\n",
        "\n",
        "    return encoded_feature"
      ],
      "metadata": {
        "id": "xAwVdUdxxQoj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Votre travail commence **ici**"
      ],
      "metadata": {
        "id": "GEmS2Hjh3DiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO Import the dataset https://github.com/mwaskom/seaborn-data/raw/master/tips.csv using Pandas\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/mwaskom/seaborn-data/raw/master/tips.csv\")\n",
        "\n",
        "### TODO Split the Pandas Dataframe into 2 parts. The features (sex, smoker, day, etc.) will go into the variable x and the column tips will go into variable y\n",
        "### Remember, what we want to predict is the value of the tip !\n",
        "\n",
        "x = data.drop('tip', axis = 1)\n",
        "y = data[\"tip\"]\n",
        "#print(x)\n",
        "\n",
        "### TODO Instantiate one object of type Input() for each variable in your features (sex, smoker, day, etc...) but not the column tips\n",
        "a = Input(shape=(1,),name=(\"sex\"),dtype=(tf.string))\n",
        "b = Input(shape=(1,),name=(\"smoker\"),dtype=(tf.string))\n",
        "c = Input(shape=(1,),name=(\"day\"),dtype=(tf.string))\n",
        "d = Input(shape=(1,),name=(\"time\"),dtype=(tf.string))\n",
        "e = Input(shape=(1,),name=(\"size\"),dtype=(tf.int32))\n",
        "f = Input(shape=(1,),name=(\"total_bill\"),dtype=(tf.float32))\n",
        "\n",
        "### In the constructor of Input(), set the shape to (1,), a name for your input, and the dtype of the input -> https://keras.io/api/layers/core_layers/input/\n",
        "### If you do not know what a dtype is, you can always look at this -> https://www.oreilly.com/library/view/tensorflow-20-quick/9781789530759/851f368a-b0af-493d-a225-150e4c719e3c.xhtml\n",
        "\n",
        "### TODO Put all of your previously instanciated inputs into a Python List called all_inputs\n",
        "\n",
        "all_inputs = [a,b,c,d,e,f]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N7ev8rfuyv4E"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMETERS GO HERE\n",
        "NUMBER_OF_KFOLDS = 2\n",
        "BATCH_SIZE = 1\n",
        "NUMBER_OF_EPOCHS = 1\n",
        "# STARTING_LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "nf7uO_Ry6UjQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO Create a variable best_model that is equal to None\n",
        "best_model = None\n",
        "\n",
        "### TODO Create a variable smallest_error that is equal to 999\n",
        "smallest_error = 999\n",
        "\n",
        "# Splits the dataset into an array of differently generated train/test subsets\n",
        "kfold = KFold(n_splits=NUMBER_OF_KFOLDS, shuffle=True)\n",
        "for train, test in kfold.split(x, y):\n",
        "    train_dataset = dataframe_to_dataset(x.iloc[train], y.iloc[train])\n",
        "    validation_dataset = dataframe_to_dataset(x.iloc[test], y.iloc[test])\n",
        "\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "    validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    ### TODO Use either the function encode_categorical_feature or encode_numerical_feature on your previously created Input objects\n",
        "    ### depending on what you think seems right\n",
        "    sex_encoded = encode_categorical_feature(a, \"sex\", train_dataset, True)\n",
        "    smoker_encoded = encode_categorical_feature(b, \"smoker\", train_dataset, True)\n",
        "    day_encoded = encode_categorical_feature(c, \"day\", train_dataset, True)\n",
        "    time_encoded = encode_categorical_feature(d, \"time\", train_dataset, True)\n",
        "    total_bill_encoded =  encode_numerical_feature(f, \"total_bill\", train_dataset)\n",
        "    size_encoded = encode_numerical_feature(e, \"size\", train_dataset)\n",
        "\n",
        "    all_features = concatenate(\n",
        "        [\n",
        "            sex_encoded,\n",
        "            smoker_encoded,\n",
        "            day_encoded,\n",
        "            time_encoded,\n",
        "            total_bill_encoded,\n",
        "            size_encoded\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    ### TODO Create your neural network\n",
        "    # Create a first Dense layer that takes as input your array of all_features\n",
        "    first_layer = tf.keras.layers.Dense(6,)(all_features)\n",
        "    \n",
        "    # Try to create a number of other layers here, see with your teacher how you can do that or refer to the course given by Mx naudin\n",
        "    second_layer = tf.keras.layers.Dense(4,)(first_layer)\n",
        "    third_layer = tf.keras.layers.Dense(2,)(second_layer)\n",
        "    # Create an output layer called output that is a Dense layer with only 1 output (this will be the tip predicted)\n",
        "    #output = tf.keras.layers.Dense(1,)(all_inputs)\n",
        "    model = Model(all_inputs, output)\n",
        "\n",
        "    # See https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
        "    model.compile(\n",
        "        # Choose an optimizer to use\n",
        "        # Choose a loss to use,\n",
        "        metrics=[\n",
        "            # Choose a metric to track\n",
        "        ]\n",
        "    )\n",
        "    model.fit(train_dataset, epochs=NUMBER_OF_EPOCHS, validation_data=validation_dataset)\n",
        "    scores = model.evaluate(validation_dataset, verbose=0)\n",
        "    ### TODO If scores[1] is less than the smallest_error variable, save your new best model in the best_model variable"
      ],
      "metadata": {
        "id": "nB0I1C226YRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "5a636f94-2709-4f22-88a4-6ef7adb5fca8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-507e213d3100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         ]\n\u001b[1;32m     51\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMBER_OF_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m### TODO If scores[1] is less than the smallest_error variable, save your new best model in the best_model variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 891, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 857, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time to evaluate the model using random data\n",
        "\n",
        "samples = [{\n",
        "    \"total_bill\": 1.,\n",
        "    \"sex\": \"Male\",\n",
        "    \"smoker\": \"Yes\",\n",
        "    \"day\": \"Mon\",\n",
        "    \"time\": \"Lunch\",\n",
        "    \"size\": 1\n",
        "},\n",
        "{\n",
        "    \"total_bill\": 16.99,\n",
        "    \"sex\": \"Female\",\n",
        "    \"smoker\": \"No\",\n",
        "    \"day\": \"Wed\",\n",
        "    \"time\": \"Dinner\",\n",
        "    \"size\": 2\n",
        "},\n",
        "{\n",
        "    \"total_bill\": 16.99,\n",
        "    \"sex\": \"Female\",\n",
        "    \"smoker\": \"Yes\",\n",
        "    \"day\": \"Wed\",\n",
        "    \"time\": \"Dinner\",\n",
        "    \"size\": 2\n",
        "},\n",
        "{\n",
        "    \"total_bill\": 10.45,\n",
        "    \"sex\": \"Male\",\n",
        "    \"smoker\": \"No\",\n",
        "    \"day\": \"Fri\",\n",
        "    \"time\": \"Lunch\",\n",
        "    \"size\": 1\n",
        "},\n",
        "{\n",
        "    \"total_bill\": 15.,\n",
        "    \"sex\": \"Female\",\n",
        "    \"smoker\": \"Yes\",\n",
        "    \"day\": \"Tue\",\n",
        "    \"time\": \"Lunch\",\n",
        "    \"size\": 2\n",
        "},\n",
        "{\n",
        "    \"total_bill\": 45.99,\n",
        "    \"sex\": \"Male\",\n",
        "    \"smoker\": \"No\",\n",
        "    \"day\": \"Sun\",\n",
        "    \"time\": \"Lunch\",\n",
        "    \"size\": 4\n",
        "}]\n",
        "\n",
        "for sample in samples:\n",
        "    input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "    predictions = best_model.predict(input_dict)\n",
        "    print(f'Pour {sample}, on prédit un pourboire de ${predictions[0][0]:.2f}')"
      ],
      "metadata": {
        "id": "4Z5TBsCr8cju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}